# transformer-network

- 1. word embedding concept
- 2. one-hot word embedding
- 3. inner product
- 4. relative similarity concept
- 5. 

# 1. Introduction to word embedding concept

Before deep diving into Transformers we need to get familiar with a basic concepts of word embedding! So, the idea is very simple; Word Embedding means that every word in our vocabualry is gonna be mapped to a vector!

<img src="./img/w2vec.png">
A firly novel way to get a deepper understanging out of it is to imagine a world map. 

<img src="./img/world.png">

We assume that the places which are geographically 